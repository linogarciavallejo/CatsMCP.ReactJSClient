# Environment Configuration for MCP React TypeScript Client

# LLM Provider API Keys
# Choose one or more providers and set the corresponding API key

# Anthropic API Configuration
# Get your API key from: https://console.anthropic.com/
VITE_ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenAI API Configuration  
# Get your API key from: https://platform.openai.com/
VITE_OPENAI_API_KEY=your_openai_api_key_here

# Ollama Configuration (for local models like DeepSeek)
# Default Ollama server URL - change if running on different host/port
VITE_OLLAMA_BASE_URL=http://localhost:11434

# MCP Server Configuration (optional defaults)
VITE_DEFAULT_SERVER_COMMAND=python
VITE_DEFAULT_SERVER_ARGS=../QuickstartWeatherServer/server.py

# Application Configuration
VITE_APP_NAME=MCP React TypeScript Client
VITE_APP_VERSION=2.0.0

# Default LLM Provider (anthropic, openai, ollama)
VITE_DEFAULT_LLM_PROVIDER=anthropic

# Default Models for each provider
VITE_DEFAULT_ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
VITE_DEFAULT_OPENAI_MODEL=gpt-4-turbo-preview
VITE_DEFAULT_OLLAMA_MODEL=deepseek-coder

# Note: In a production environment, API keys should be handled securely
# through a backend service, not exposed in the frontend.

# For local development with Ollama + DeepSeek:
# 1. Install Ollama: https://ollama.ai/
# 2. Pull DeepSeek model: ollama pull deepseek-coder
# 3. Start Ollama: ollama serve
# 4. Set REACT_APP_DEFAULT_LLM_PROVIDER=ollama
